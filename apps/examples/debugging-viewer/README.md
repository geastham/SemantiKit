# RAG Debugging Viewer - Visualize Retrieval Paths

A debugging tool for RAG (Retrieval-Augmented Generation) systems that visualizes the retrieval process and shows how documents and chunks are selected.

## ğŸ¯ Features

### Retrieval Trace Visualization
- Show query â†’ retrieval â†’ ranking â†’ selection flow
- Display vector similarity scores at each step
- Visualize re-ranking steps with explanations
- Show final context window sent to LLM

### Document Chunk Explorer
- View all chunks for a document
- See embedding metadata and dimensions
- Display chunk relationships and overlap
- Filter by similarity threshold

### Query Analysis
- Show query embedding visualization
- Display nearest neighbors in vector space
- Compare different retrieval strategies side-by-side
- A/B test different parameters (top-k, threshold, etc.)

### Performance Metrics
- Latency breakdown (embedding, search, ranking)
- Retrieval quality scores (precision, recall)
- Coverage analysis (% of documents used)
- Cost tracking (API calls, tokens)

### Interactive Playground
- Test queries in real-time
- Adjust retrieval parameters dynamically
- Compare retrieval methods (vector, keyword, hybrid)
- Export debug reports for sharing

## ğŸš€ Quick Start

```bash
cd apps/examples/debugging-viewer
pnpm install
pnpm dev
```

Visit `http://localhost:3004`

## ğŸ“– Usage

### Debugging a RAG Query

1. **Enter Query**
```
"What is the Paris Agreement?"
```

2. **View Retrieval Trace**
```
[Query] â†’ [Embed] â†’ [Vector Search] â†’ [Re-rank] â†’ [LLM]
  200ms     50ms       100ms           30ms      1.2s
```

3. **Inspect Retrieved Chunks**
```
1. doc_15_chunk_3  Score: 0.89  Rank: 1
   "The Paris Agreement... signed in 2015..."
   
2. doc_22_chunk_7  Score: 0.84  Rank: 3 â†’ 2
   "Climate treaty frameworks include..."
```

4. **Analyze Performance**
- Total latency: 1.58s
- Chunks retrieved: 50
- Chunks selected: 5
- Cost: $0.002

### Comparing Strategies

Test different retrieval approaches:

```typescript
const strategies = [
  { name: 'Vector Only', method: 'vector', topK: 5 },
  { name: 'Hybrid', method: 'hybrid', topK: 10, alpha: 0.7 },
  { name: 'Keyword', method: 'keyword', topK: 5 }
];
```

### Performance Profiling

```typescript
const profile = {
  embedding: {
    provider: 'OpenAI',
    model: 'text-embedding-3-small',
    latency: 50,
    cost: 0.0001
  },
  search: {
    engine: 'Pinecone',
    latency: 100,
    candidates: 50
  },
  reranking: {
    model: 'cohere-rerank-v3',
    latency: 30,
    cost: 0.001
  }
};
```

## ğŸ—ï¸ Architecture

### Tech Stack

- **Framework:** Next.js 14
- **Graph Viz:** React Flow + SemantiKit, D3.js
- **Charts:** Recharts, Visx
- **UI:** Tailwind CSS, Headless UI
- **State:** React Query

### Project Structure

```
debugging-viewer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx              # Main debugger
â”‚   â”‚   â”œâ”€â”€ compare/page.tsx      # Strategy comparison
â”‚   â”‚   â”œâ”€â”€ metrics/page.tsx      # Performance dashboard
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â”œâ”€â”€ trace/route.ts
â”‚   â”‚       â””â”€â”€ analyze/route.ts
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ TraceVisualization.tsx
â”‚   â”‚   â”œâ”€â”€ ChunkExplorer.tsx
â”‚   â”‚   â”œâ”€â”€ QueryAnalyzer.tsx
â”‚   â”‚   â”œâ”€â”€ MetricsDashboard.tsx
â”‚   â”‚   â””â”€â”€ ComparisonView.tsx
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ trace-recorder.ts
â”‚       â”œâ”€â”€ similarity-calculator.ts
â”‚       â””â”€â”€ performance-analyzer.ts
â””â”€â”€ package.json
```

## ğŸ”Œ Integration

### Record Traces

Instrument your RAG pipeline:

```typescript
import { traceRecorder } from '@/lib/trace-recorder';

const trace = traceRecorder.start();

// Your RAG pipeline
const embedding = await embed(query);
trace.record('embedding', { latency, cost });

const results = await vectorSearch(embedding);
trace.record('search', { results, latency });

const reranked = await rerank(results);
trace.record('rerank', { reranked, latency });

trace.finish();
```

### Export Traces

```bash
# Export as JSON
GET /api/trace/export?format=json

# Export as CSV
GET /api/trace/export?format=csv
```

## ğŸ“Š Metrics Tracked

### Latency Metrics
- Query embedding time
- Vector search time
- Re-ranking time
- Total end-to-end latency

### Quality Metrics
- Retrieval precision@k
- Retrieval recall@k
- Re-ranking improvement
- Context relevance score

### Cost Metrics
- Embedding API costs
- Vector DB query costs
- Re-ranking costs
- Total cost per query

### Coverage Metrics
- Documents retrieved (%)
- Unique documents used
- Chunk distribution
- Temporal coverage

## ğŸ¯ Use Cases

- **Debug RAG Performance** - Identify bottlenecks
- **Optimize Retrieval** - Test different strategies
- **Quality Assurance** - Validate retrieval accuracy
- **Cost Analysis** - Track and reduce API costs
- **A/B Testing** - Compare retrieval methods

## ğŸ”§ Configuration

### Vector Database

Connect to your vector DB:

```typescript
const config = {
  provider: 'pinecone', // or 'weaviate', 'qdrant', 'milvus'
  apiKey: process.env.PINECONE_API_KEY,
  index: 'my-index',
  namespace: 'default'
};
```

### Embedding Models

Configure embedding provider:

```typescript
const embedding = {
  provider: 'openai', // or 'cohere', 'huggingface'
  model: 'text-embedding-3-small',
  dimensions: 1536
};
```

## ğŸ“¦ Deployment

Deploy to Vercel:

```bash
vercel deploy --prod
```

### Environment Variables

```bash
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
COHERE_API_KEY=...
```

## ğŸ¤ Contributing

See [CONTRIBUTING.md](../../../CONTRIBUTING.md)

## ğŸ“ License

MIT License - Part of SemantiKit

## ğŸ”— Links

- [RAG Best Practices](https://docs.semantikit.dev/advanced/rag)
- [Performance Optimization](https://docs.semantikit.dev/advanced/performance)
- [Vector DB Comparison](https://docs.semantikit.dev/guides/vector-dbs)

---

**Live Demo:** [debugger.semantikit.dev](https://debugger.semantikit.dev) (Coming soon)

**Status:** ğŸ¯ Stretch Goal (Post v1.0.0 release)

